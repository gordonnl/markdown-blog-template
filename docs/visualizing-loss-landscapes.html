<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, minimal-ui, initial-scale=1, viewport-fit=cover">
    <title>Visualizing Neural Network Loss Landscapes</title>
    <!-- <link rel="icon" type="image/png" href="assets/images/favicon.png"> -->
    <link href="assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="assets/css/skeleton.css">
</head>

<nav>
    <a class="navigation" href=".">home</a>
    <a class="navigation" href="resume.html">resume</a>
    <a class="navigation" href="projects.html">projects</a>
</nav>

<body>
    <h1 id="visualizing-neural-network-loss-landscapes">Visualizing Neural Network Loss Landscapes</h1>
<h3 id="january-2023">January 2023</h3>
<p></p>
<p><img src="../posts/visualizing-loss-landscapes/images/ResNet-PCA-3d-contours.png" alt="QR Codes">
<p style="text-align: center; margin-top:-5px"><i>The path to convergence on a loss landscape.</i></p>
<br></p>
<p>Training artificial neural networks is a sensitive process due to architecture choices and hyperparameter tuning. By visualizing both the model’s loss landscape and path during gradient descent towards some local minimum, we can gain intuitions about how tuning and architectural decisions impact the model’s ability to converge. In this project, Chase Ison and I visualize loss landscapes of a convolutional neural network by implementing two separate methods proposed by Li et. al. in &quot;<a href="https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html">Visualizing the Loss Landscape of Neural Nets</a>.&quot; For each method, we perform dimensionality reduction on a model’s weights during backpropagation, then iteratively manipulate the weights using these techniques to generate scalar fields termed &#39;loss landscapes&#39;. Finally, we evaluate our results on two popular neural network models: ResNet-50 and VGG-11.</p>
<p>The project was the open-ended final for CS 453 - Scientific Visualization at OSU in fall 2022. This post will just be a quick overview of the project and some of our results.</p>
<br>

<h4 id="solution-overview">Solution Overview</h4>
<p>We chose to visualize two convolutional neural networks: ResNet-50 and VGG, each trained to classify images of the CIFAR-10 dataset. For each model, we generated two different loss landscapes from the two techniques detailed by Hao Li et al: <em>random direction iteration</em> and <em>principle component analysis (PCA)</em>.</p>
<p><img src="../posts/visualizing-loss-landscapes/images/cifar-10-dataset-wide.png" alt="CIFAR-10 dataset examples">
<p style="text-align: center; margin-top:-5px"><i>CIFAR-10 dataset examples.</i></p>
</p>
<p>Our process for generating the visualization went as follows:</p>
<ol>
<li>Train a model.</li>
<li>Perform dimensionality reduction to two vectors using either the random direction approach or PCA approach.</li>
<li>Iterate over a 25x25 square of points across the plane of those two vectors, calculate the loss at each point, saving each (x, y, loss) tuple to our results matrix.</li>
<li>Export the results to a PLY file.</li>
<li>Visualize the PLY file in OpenGL, using different techniques to produce different visualizations, such as marching squares to visualize contour lines.</li>
</ol>
<p>You might be wondering why we use two different versions of dimensionality reduction.
Random direction iteration is essentially the simple way to generate a loss landscape, which was nice to do first just to make sure we were getting a good result.
On the other hand, PCA gave us meaningful insights and allowed us to trace the path of the model converging as it minimized the loss.</p>
<br>

<h4 id="results">Results</h4>
<p><img src="../posts/visualizing-loss-landscapes/images/paper-teaser.png" alt="QR Codes">
<p style="text-align: center; margin-top:-5px"><i>Loss landscapes of the ResNet-50 model from PCA dimensionality reduction, with the gradient decent path.</i></p>
<br></p>
<p>The variations of loss landscapes above show that a single picture might not illustrate the 3D topography understandably.
Our OpenGL code allowed the user to scroll the point-of-view camera around the loss landscape, and change the texture with the click of a button.
The visualization below highlights local minima that the model may get stuck in while training with too small of a learning rate.</p>
<p><img src="../posts/visualizing-loss-landscapes/images/ResNet-Random-3d-contours-critical-points.png" alt="QR Codes">
<p style="text-align: center; margin-top:-5px"><i>The 3D loss landscape with critical points labelled for the ResNet-50 model using random direction iteration for dimensionality reduction.</i></p>
<br></p>
<h4 id="key-takeaways">Key Takeaways</h4>
<p>This project pushed me to understand dimensionality reduction as well as what we can learn from loss landscapes. It stressed a lot of different skills- being able to write PyTorch code, being able to write dimensionality reduction code, train the models on a high-performance cluster, format the data, and visualize it with custom OpenGL code. I&#39;m thankful to have had Chase as my project partner to collaborate with!</p>
<p><br><a href="index.html">back to home</a></p>

</body>

</html>